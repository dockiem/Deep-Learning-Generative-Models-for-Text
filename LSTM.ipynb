{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models for Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) In this problem, we are trying to build a generative model to mimic the writing\n",
    "### style of prominent British Mathematician, Philosopher, prolific writer, and\n",
    "### political activist, Bertrand Russell.\n",
    "### (b) Download the following books from Project Gutenberg http://www.gutenberg.\n",
    "### org/ebooks/author/355 in text format:\n",
    "### i. The Problems of Philosophy\n",
    "### ii. The Analysis of Mind\n",
    "### iii. Mysticism and Logic and Other Essays\n",
    "### iv. Our Knowledge of the External World as a Field for Scientific Method in\n",
    "### Philosophy\n",
    "### Project Gutenberg adds a standard header and footer to each book and this is\n",
    "### not part of the original text. Open the file in a text editor and delete the header\n",
    "### and footer.\n",
    "### The header is obvious and ends with the text:\n",
    "### *** START OF THIS PROJECT GUTENBERG EBOOK AN INQUIRY INTO\n",
    "### MEANING AND TRUTH ***\n",
    "### The footer is all of the text after the line of text that says:\n",
    "### THE END\n",
    "### To have a better model, it is strongly recommended that you download the following\n",
    "### books from The Library of Congress https://archive.org and convert\n",
    "### them to text files:\n",
    "### i. The History of Western Philosophy\n",
    "### https://archive.org/details/westernphilosophy4\n",
    "### ii. The Analysis of Matter\n",
    "### https://archive.org/details/in.ernet.dli.2015.221533\n",
    "### iii. An Inquiry into Meaning and Truth\n",
    "### https://archive.org/details/BertrandRussell-AnInquaryIntoMeaningAndTruth\n",
    "### Try to only use the text of the books and throw away unwanted text before and\n",
    "### after the text, although in a large corpus, these are considered as noise and should\n",
    "### not make big problems.1\n",
    "### (c) LSTM: Train an LSTM to mimic Russell’s style and thoughts:\n",
    "### i. Concatenate your text files to create a corpus of Russell’s writings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = open('corpus_raw.txt').read()\n",
    "text_data = text_data.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Use a character-level representation for this model by using extended ASCII\n",
    "### that has N = 256 characters. Each character will be encoded into a an integer\n",
    "### using its ASCII code. Rescale the integers to the range [0, 1], because LSTM\n",
    "### uses a sigmoid activation function. LSTM will receive the rescaled integers\n",
    "### as its input.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = sorted(list(set(text_data)))\n",
    "int_char = dict((c, i) for i, c in enumerate(characters))\n",
    "characters_number = len(text_data)\n",
    "vocab_number = len(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Choose a window size, e.g., W = 100.\n",
    "### iv. Inputs to the network will be the first W −1 = 99 characters of each sequence,\n",
    "### and the output of the network will be the Wth character of the sequence.\n",
    "### Basically, we are training the network to predict each character using the 99\n",
    "### characters that precede it. Slide the window in strides of S = 1 on the text.\n",
    "### For example, if W = 5 and S = 1 and we want to train the network with the\n",
    "### sequence ABRACADABRA, The first input to the network will be ABRA\n",
    "### and the corresponding output will be C. The second input will be BRAC and\n",
    "### the second output will be A, etc.\n",
    "### v. Note that the output has to be encoded using a one-hot encoding scheme with\n",
    "### N = 256 (or less) elements. This means that the network reads integers, but\n",
    "### outputs a vector of N = 256 (or less) elements.\n",
    "### vi. Use a single hidden layer for the LSTM with N = 256 (or less) memory units.\n",
    "### vii. Use a Softmax output layer to yield a probability prediction for each of the\n",
    "### characters between 0 and 1. This is actually a character classification problem\n",
    "### with N classes. Choose log loss (cross entropy) as the objective function for\n",
    "### the network (research what it means).3\n",
    "### viii. We do not use a test dataset. We are using the whole training dataset to\n",
    "### learn the probability of each character in a sequence. We are not seeking for\n",
    "### a very accurate model. Instead we are interested in a generalization of the\n",
    "### dataset that can mimic the gist of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_window = 100\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(0, characters_number - sequence_window, 1):\n",
    "    seq_in = text_data[i:i + sequence_window]\n",
    "    seq_out = text_data[i + sequence_window]\n",
    "    X.append([int_char[char] for char in seq_in])\n",
    "    Y.append(int_char[seq_out])\n",
    "number_of_patterns = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(Y)\n",
    "X = numpy.reshape(X, (number_of_patterns, sequence_window, 1))\n",
    "X = X / float(vocab_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ix. Choose a reasonable number of epochs4\n",
    "### for training, considering your computational\n",
    "### power (e.g., 30, although the network will need more epochs to yield\n",
    "### a better model).\n",
    "### x. Use model checkpointing to keep the network weights to determine each time\n",
    "### an improvement in loss is observed at the end of the epoch. Find the best set\n",
    "### of weights in terms of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "844780/844780 [==============================] - 8000s 9ms/step - loss: 3.0946\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.09463, saving model to weights-improvement-01-3.0946.hdf5\n",
      "Epoch 2/10\n",
      "844780/844780 [==============================] - 8086s 10ms/step - loss: 2.9827\n",
      "\n",
      "Epoch 00002: loss improved from 3.09463 to 2.98274, saving model to weights-improvement-02-2.9827.hdf5\n",
      "Epoch 3/10\n",
      "844780/844780 [==============================] - 8095s 10ms/step - loss: 2.9044\n",
      "\n",
      "Epoch 00003: loss improved from 2.98274 to 2.90442, saving model to weights-improvement-03-2.9044.hdf5\n",
      "Epoch 4/10\n",
      "844780/844780 [==============================] - 8395s 10ms/step - loss: 2.8678\n",
      "\n",
      "Epoch 00004: loss improved from 2.90442 to 2.86781, saving model to weights-improvement-04-2.8678.hdf5\n",
      "Epoch 5/10\n",
      "844780/844780 [==============================] - 8086s 10ms/step - loss: 2.8335\n",
      "\n",
      "Epoch 00005: loss improved from 2.86781 to 2.83347, saving model to weights-improvement-05-2.8335.hdf5\n",
      "Epoch 6/10\n",
      "844780/844780 [==============================] - 7982s 9ms/step - loss: 2.8083\n",
      "\n",
      "Epoch 00006: loss improved from 2.83347 to 2.80833, saving model to weights-improvement-06-2.8083.hdf5\n",
      "Epoch 7/10\n",
      "844780/844780 [==============================] - 8316s 10ms/step - loss: 2.7879\n",
      "\n",
      "Epoch 00007: loss improved from 2.80833 to 2.78792, saving model to weights-improvement-07-2.7879.hdf5\n",
      "Epoch 8/10\n",
      "844780/844780 [==============================] - 8137s 10ms/step - loss: 2.7677\n",
      "\n",
      "Epoch 00008: loss improved from 2.78792 to 2.76769, saving model to weights-improvement-08-2.7677.hdf5\n",
      "Epoch 9/10\n",
      "844780/844780 [==============================] - 8126s 10ms/step - loss: 2.7512\n",
      "\n",
      "Epoch 00009: loss improved from 2.76769 to 2.75118, saving model to weights-improvement-09-2.7512.hdf5\n",
      "Epoch 10/10\n",
      "844780/844780 [==============================] - 8080s 10ms/step - loss: 2.7349\n",
      "\n",
      "Epoch 00010: loss improved from 2.75118 to 2.73488, saving model to weights-improvement-10-2.7349.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb22eb3588>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(X, y, epochs=10, batch_size=4096, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xi. Use the network with the best weights to generate 1000 characters, using the\n",
    "### following text as initialization of the network:\n",
    "### There are those who take mental phenomena naively, just as they\n",
    "### would physical phenomena. This school of psychologists tends not to\n",
    "### emphasize the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-10-2.7349.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "int_to_char = dict((i, c) for i, c in enumerate(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" there are those who take mental phenomena naively, just as they would physical phenomena. this schoo \"\n",
      "little tare gereen to be a gentle of the there are those who take mental phenomena naively, just as they would physical phenomena. this schoo tabdit  soenee the gad  ouw ie the tay a tirt of toiet at the was a little  anonersen, and thiu had been woite io a lott of tueh a tiie  and taede bot her aeain  she cere thth the bene tith the tere bane to tee toaete to tee the harter was a little tire the same oare cade an anl ano the garee and the was so seat the was a little gareen and the sabdit, and the white rabbit wese tilel an the caoe and the sabbit se teeteer, and the white rabbit wese tilel an the cade in a lonk tfne the sabdi ano aroing to tea the was sf teet whitg the was a little tane oo thete the sabeit  she was a little tartig to the tar there are those who take mental phenomena naively, just as they would physical phenomena. this schoo tf tee the tame of the cagd, and the white rabbit was a little toiee to be anle tite thete ofs and the tabdit was the wiite rabbit, and\n"
     ]
    }
   ],
   "source": [
    "p = \"There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.\".lower()\n",
    "pattern = [char_to_int[char] for char in p]\n",
    "pattern = pattern[0:100]\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_number)\n",
    "    pred = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(pred)\n",
    "    result = int_to_char[index]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
